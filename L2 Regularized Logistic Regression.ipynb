{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a567d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as cma\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold as kf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49b0d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f6d76534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, n_iters=1000, alpha=1):\n",
    "        self.lr = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history= [ ]\n",
    "        self.w_list = [ ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # initializing the parameters:\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        eps=10**-10\n",
    "        \n",
    "        # gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            \n",
    "            # approximate y with linear combination of weights and x, plus bias\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # apply sigmoid function\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "            \n",
    "            # compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # update parameters:\n",
    "            w_prev = self.weights\n",
    "            w0_prev = self.bias\n",
    "            self.weights = self.weights*(1 - self.alpha * self.lr/n_samples) - self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            \n",
    "            #appending cost and w values\n",
    "            \n",
    "            cost = -(np.sum( y * np.log(y_predicted) + (1-y) * np.log(1 - y_predicted)) +  self.alpha*np.sum(np.square(self.weights)))\n",
    "            \n",
    "            w=list(self.weights)\n",
    "            w.append(self.bias)\n",
    "            self.w_list.append(w)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            if np.sum(np.square(w_prev-self.bias))<eps:\n",
    "                break\n",
    "            \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        return y_predicted\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def score(self,x_test,y_test):\n",
    "        y_p = self.predict(x_test)\n",
    "        correct = 0\n",
    "        for i in range(len(y_p)):\n",
    "            if y_p[i]==y_test[i]:\n",
    "                correct+=1\n",
    "        return correct/len(y_p)\n",
    "    \n",
    "    def error(self,x_test,y_test):\n",
    "        y_p = self.predict(x_test)\n",
    "        return np.sum(np.square(y_p-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "579a9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(learning_rate=0.01,n_iters=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
